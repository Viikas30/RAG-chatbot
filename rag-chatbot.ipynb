{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10448631,"sourceType":"datasetVersion","datasetId":6467519},{"sourceId":10449972,"sourceType":"datasetVersion","datasetId":6468477}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"All the required libraries","metadata":{}},{"cell_type":"code","source":"# Install required libraries\n!pip install langchain pinecone-client huggingface_hub python-dotenv -U langchain-community sentence-transformers\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install --upgrade tensorflow transformers\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Implementing vector database and LLM chain using Langchain","metadata":{}},{"cell_type":"code","source":"from langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom sentence_transformers import SentenceTransformer\nfrom langchain.vectorstores import Pinecone as LangChainPinecone\nfrom langchain.embeddings.base import Embeddings\nfrom langchain.llms import HuggingFaceHub\nfrom pinecone import Pinecone, ServerlessSpec\nfrom dotenv import load_dotenv\nimport os\nfrom langchain import PromptTemplate\nfrom langchain.chains import LLMChain\nimport numpy as np\nfrom langchain_google_genai import GoogleGenerativeAI\n\nclass CustomEmbedding(Embeddings):\n    \"\"\"Custom Embedding class wrapping SentenceTransformer.\"\"\"\n\n    def __init__(self, model_name: str):\n        self.embedding_model = SentenceTransformer(model_name)\n\n    def embed_documents(self, texts):\n        embeddings = self.embedding_model.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n        # Ensure embeddings are returned as a list of lists\n        return embeddings.tolist()\n\n    def embed_query(self, text):\n        embedding = self.embedding_model.encode([text], convert_to_numpy=True)[0]\n        return embedding.tolist()\n\nclass ChatBot():\n    def __init__(self):\n        # Load environment variables\n        load_dotenv()\n\n        # Step 1: Load and Split Documents\n        loader = TextLoader('/kaggle/input/haircare/haircare.txt')\n        documents = loader.load()\n        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=4)\n        docs = text_splitter.split_documents(documents)\n\n        # Step 2: Setup Sentence Transformers for Embeddings\n        model_name = \"sentence-transformers/all-distilroberta-v1\"  # 384-dimensional output\n        embeddings = CustomEmbedding(model_name)\n\n        # Step 3: Initialize Pinecone instance\n        pc = Pinecone(pinecone_api_key1)\n        index_name = \"hair-index\"\n\n\n\n        # Step 5: Create Pinecone Index with dimension 384\n        if index_name not in pc.list_indexes().names():\n            pc.create_index(\n                name=index_name,\n                dimension=768,  # Match SentenceTransformer embedding dimension\n                metric=\"cosine\",\n                spec=ServerlessSpec(\n                    cloud=\"aws\",  # Specify your cloud provider\n                    region=\"us-east-1\"  # Specify your region\n                )\n            )\n\n        # Step 6: Connect LangChain Pinecone and documents\n        self.docsearch = LangChainPinecone.from_documents(\n            docs,\n            embeddings,\n            index_name=index_name\n        )\n\n        repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n        self.llm = HuggingFaceHub(\n            repo_id=repo_id,\n            model_kwargs={\"temperature\": 0.8, \"top_p\": 0.8, \"top_k\": 50},\n            huggingfacehub_api_token=huggingface_api_token1)      # Step 8: Define Prompt Template\n        self.template =\"\"\"\n  You are a Hairstylist. These Human will ask you a questions about their Hair. Use following piece of context to answer the question. \n  If you don't know the answer, just say you don't know. \n  You answer with short and concise answer, no longer than2 sentences.\n\n  Context: {context}\n  Question: {question}\n  Answer: \n\n  \"\"\"\n        self.prompt = PromptTemplate(template=self.template, input_variables=[\"context\", \"question\"])\n\n        # Step 9: Set up LLM Chain for RAG (Retrieval-Augmented Generation)\n        self.chain = LLMChain(\n            llm=self.llm,\n            prompt=self.prompt\n        )\n\n    # Method to run the chatbot with a user query\n    def get_answer(self, question):\n        # Perform similarity search in Pinecone to retrieve relevant documents\n        docs = self.docsearch.similarity_search(query=question, k=5)\n        \n        # If no relevant documents, return \"I don't know.\"\n        if not docs:\n            return \"I don't know.\"\n        \n        # Combine documents into context\n        context = \" \".join([doc.page_content for doc in docs])\n\n        # Run the LLMChain with the context and question\n        result = self.chain.run({\"context\": context, \"question\": question})\n\n        return result\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Taking user Query","metadata":{}},{"cell_type":"code","source":"# Initialize the chatbot\nchatbot = ChatBot()\n\n# Example Test Query\nuser_question = \"hair cuts for straight hair tips\"\nresponse = chatbot.get_answer(user_question)\n\n# Output the result\nprint(f\"Answer to question '{user_question}': {response}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Required APIs","metadata":{}},{"cell_type":"code","source":"import os\n\nos.environ[\"PINECONE_API_KEY\"] = \"\"\nos.environ[\"HUGGINGFACE_API_TOKEN\"] = \"\"\n\n# Access the variables\npinecone_api_key1 = os.getenv(\"PINECONE_API_KEY\")\nhuggingface_api_token1 = os.getenv(\"HUGGINGFACE_API_TOKEN\")\n\n# Verify\nprint(f\"Pinecone API Key: {pinecone_api_key1}\")\nprint(f\"Hugging Face API Token: {huggingface_api_token1}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}